%!TEX root = ../main.tex
\section{Method}
\subsection{Rolling correlations}
We compute rolling correlation estimates in order to investigate whether the factor equity strategies exhibit constant correlation over time. 
\begin{align}
    RCorr(r_i, r_j)_t^{250} = \frac{\sum^{t}_{t-249}(r_i - \bar{r}_i)(r_j - \bar{r}_j)}{\sqrt{\sum^{t}_{t-249} (r_i - \bar{r}_i)^2} \sqrt{\sum^{t}_{t-249} (r_j - \bar{r}_j)^2}}
\end{align}
where $r_i$, $r_j$ represent the $N \cdot (N-1) / 2$ different pairs of the factor strategies' log returns and the window of interest is set to approximate one trading year (250 days).
\subsection{Threshold correlations}
Threshold (or exceedance) correlations have previously been used to highlight the asymmetric dependence structure of i.a. country equity indices~\autocite{LonginSolnik2001}, portfolios by industry, size, value and momentum~\autocite{AngChen2002} and factor strategies~\autocite{ChristoffersenLanglois2013}. Still, however, the following analysis is incremental, as it adds the new factors profitability (RMW) and investment (CMA). We follow~\textcite{ChristoffersenLanglois2013} definition of threshold correlation
\begin{align}
    TCorr(r_i, r_j) = 
    \begin{cases} 
        Corr\Big(r_i, r_j \,|\, r_i < F_i^{-1}(p), r_j < F_j^{-1}(p)\Big)  & \text{for } p < 0.5 \\
        Corr\Big(r_i, r_j \,|\, r_i \geq F_i^{-1}(p), r_j \geq F_j^{-1}(p)\Big)  & \text{for } p \geq 0.5
    \end{cases}
\end{align}
where $F_i^{-1}(p)$ the empirical quantile of $r_i$ at percentile $p$. Graphically, threshold correlation are easily understood as the standard correlation estimate in more and more distant parts of the first and third quadrant as $p$ approaches either zero or unity. Ceteris paribus, assets pairs with weaker or negative threshold correlation as $p < 0.50$ are better diversified, as they do not coincide and contribute to negative skewness of the portfolio as a whole. Threshold correlations capture a dimension of dependence that is overlooked by the linear correlation statistic.\footnote{Only in the case of elliptical distributions is zero correlation equivalent to independence.} 
\subsection{Copula}
Copulae provide a numerically feasible way to estimate a multivariate distribution function, which can then be used to draw inferences and simulate return series.~\textcite{Patton2006} uses the theorem of~\textcite{Sklar1959} to show that the conditional multivariate distribution function of log returns can be decomposed into a copula function and the product of univariate distributions
\begin{align} \label{eq:sklar}
    f_t(r_{1,t+1}, ..., r_{N, t+1}) &= c_t(u_{1, t+1}, ... u_{N, t+1}) \prod^N_{i=1} f_{i,t}(r_{i, t+1})
\end{align}
where $c_t(u_{1, t+1}, ... u_{N, t+1})$ is the copula density function, in this application taking uniform transformations of residuals from the AR-GARCH model, $\{u_i\}$, as arguments. This relies on a two-step maximum likelihood procedure (also known as the inference-by-margins (IFM) procedure) introduced by~\textcite{Joe1997}, which starts by estimating with the marginal distributions and subsequently estimates the copula function, using the margin residuals as given. For details on the marginal estimation procedure, see \autoref{App:AppendixB} The IFM method drastically improves the speed of the optimization process in large data sets; however, it is not efficient as shown by~\textcite{ChenFanTsyrennikov2006}, who instead propose a sieve ML estimation. This paper still employs IFM, as the efficiency loss is small in most cases~\autocite{Patton2006}.  

We focus on a skewed Student-\textit{t} copula, parameterized by $\Theta = \{\gamma, \nu, R\}$ using the constant correlation specification, and by $\Theta^{cDCC} = \{\alpha, \beta, \gamma, \nu, R\}$ using the dynamic correlation process. The analysis closely follows in the footsteps of~\textcite{Aielli2013} and~\textcite{ChristoffersenErrunzaJacobLanglois2012}. The skewed Student-\textit{t} distribution is described in detail in \autoref{App:AppendixA}.

The normal and Student-\textit{t} copulae are both nested in the skewed Student-\textit{t} model, as the degree of freedom and skewness parameters go to infinity and zero respectively. Results are presented for all three copulae, with and without correlation dynamics.

\subsection{Constant correlation specification}
As a benchmark model, we consider the case of a constant correlation matrix between the factor strategies' ARMA-GJR-GARCH residuals.  

With a constant correlation matrix $R$, the copula parameters $\Theta = \{\gamma, \nu, R\}$ are estimated using ML of the copula function, where $\gamma$ is the vector of skewness parameters and $\nu$ is the degree of freedom of the skewed Student-\textit{t} distribution. Rearranging Sklar's Theorem in \autoref{eq:sklar} and taking logs, the copula log-likelihood function is
\begin{align} \label{eq:constantllf}
    LLF(\gamma, \nu, R; u_1, ..., u_T) = \sum^T_{t=1} \Big \{ ln f_t(\varepsilon_{t}; \gamma, \nu, R) - \sum^N_{i = 1} ln f_{i,t}(\varepsilon_{i, t}; \gamma, \nu) \Big \}
\end{align}
where the density function $f$ is the skewed Student-\textit{t} given by \autoref{eq:dskewt}.

\subsection{\textit{c}DCC conditional correlation process}
To capture time-varying dependency, as motivated by rolling correlation and threshold correlation analyses, the copula correlation matrix $R_t$ is allowed to vary over time according to the \textit{c}DCC model~\autocite{Aielli2013}\footnote{\textit{c} stands for corrected, as Aielli (2009) has shown that the standard DCC estimator of Engle (2002) and Tse \& Tsui (2002) can be inconsistent.}
\begin{align} \label{eq:qtrtlink}
    R_t &= Q_t^{-1/2} Q_t Q_t^{-1/2}
    \intertext{where the core process is}
    Q_t &= (1 - \alpha - \beta) S + \alpha z_{t-1} z_{t-1}^\top + \beta Q_{t-1}
\end{align}
The $Q_t$ process is comprised of three components that are weighted according to $\alpha, \beta$: (1) $S$, a time-invariant component, to be interpreted similarly to a long-term mean in a regular GARCH process as $E[Q_t] = S$, (2) $z_{t-1} z_{t-1}^\top$, an innovation component from the copula shocks, and (3) $Q_{t-1}$, an autoregressive component of order one. In order for the the correlation matrix $R_t$ to be positive definite, $Q_t$ has to be positive definite, which is ascertained by requiring that $\alpha \geq 0$, $\beta \geq 0$ and $(\alpha + \beta) < 1$.

The parameters are simultaneously estimated with the skewed Student-\textit{t} parameters $\nu$ and $\gamma$ using ML for the copula function, whose log-likelihood function is again found by rearranging Sklar's theorem (\autoref{eq:sklar}) and taking logs on both sides
\begin{align} \label{eq:cdccllf}
    LLF(\alpha, \beta, \gamma, \nu; u_1, ..., u_T) = \sum^T_{t=1} \Big \{ ln f_t(\varepsilon_{t}; \alpha, \beta, \gamma, \nu, R) - \sum^N_{i = 1} ln f_{i,t}(\varepsilon_{i, t}; \gamma, \nu) \Big \}
\end{align}
where the density function $f$ is the skewed Student-\textit{t} given by \autoref{eq:dskewt}.

The process of the copula estimation with \textit{c}DCC dynamics is quite involved, as it relies on moment matching and recursive estimation of parameters to estimate the copula correlation matrices $\{\hat{R}\}$. Again, we follow \textcite{Aielli2013}, as is described in further detail in \autoref{App:AppendixD}.

\subsection{VaR and ES estimation}
Throughout this paper, we will discuss VaR and ES as the percentage VaR and percentage ES, i.e. the $1-\alpha \%$ VaR is the percentage return of asset $i$ that is expected to be exceeded only $\alpha \%$ of the time, and the $1 -\alpha \%$ ES is the expected return of asset $i$ when it realizes below the $1 - \alpha \%$ VaR. 

\textbf{Analytical VaR and ES}

The baseline risk model of VaR and ES is analytical and uses the sample variance-covariance matrix, and assumes normal return distributions.
\begin{align}
    VaR_i^{1-\alpha} &= \bar{\sigma}_i \cdot Z_{\alpha} \\
    ES_i^{1-\alpha} &= E[r_i | r_i < VaR_i^{1-\alpha}]
\end{align}
where for asset $i$, $\bar{\sigma}_i$ is the sample standard deviation of the return $r_i$, and $Z$ is the $\alpha \%$ quantile of the normal distribution.

While the analytical VaR and ES are simple to calculate, they are limited by the assumption of a probability distribution and the estimation of the corresponding parameters. Furthermore, we note that the estimates are constant. In reality, the probability distribution is likely to be non-stationary, as shown by volatility clustering effects in the data.

\textbf{Historically simulated VaR and ES}

For asset i, let the historically simulated VaR be
\begin{align}
    VaR_{i,T+1}^{1-\alpha} &= F_{i, T+1}^{-1}(\alpha | I_T) \\
    ES_{i, T+1}^{1 - \alpha} &= E_t[r_{i,T+1} | r_{i,T+1} < VaR_{i,T+1}^{1-\alpha}]
\end{align}
where $F_{i, T+1}^{-1}(\alpha | I_T)$ is the inverse empirical CDF (based on historical return data set $I_t$) of asset $i$ at time $T+1$.

The historically simulated VaR and ES are no longer bound by the correct parameterization of returns. Furthermore, the estimates can vary over time as new information is incorporated in the information set. However, the risk is that future asset returns have little to do with the past. The most naÃ¯ve version of historical simulation fails to incorporate even simple trends in the data, such as time-varying average volatility. As volatility in financial return series is generally persistent, returns from a low volatility regime have lower predictive power of returns in a high volatility regime. We improve the historical simulation by following \textcite{HullWhite1998}, who propose to volatility update returns by scaling historical returns to the current conditional volatility. The volatility updated returns $\{r^*_i\}$ are calculated as
\begin{align}
    r^*_{i, t} = \sigma_{i,T} \cdot \frac{r_{i,t}}{\sigma_{i,t}}
\end{align}
where $\sigma_{i,T}$ is the conditional estimate of volatility at time $T$ made at time $T-1$ using the GARCH model and $\sigma_{i,t}$ is the conditional estimate of volatility at time $t$ made at time $t-1$ using the GARCH model, and $t<T$.

\textbf{Multivariate EWMA}

[PENDING]

\textbf{Copula-based VaR}

As our analysis of GARCH residuals and threshold correlations highlights, the dependence between factor return series can not be captured by linear correlations and elliptical dependency. The only model for VaR and ES that can generate these dynamics is the copula model.

By simulating many runs of one-week-ahead copula shocks, we can find the simulated probability distributions for one-week-ahead factor strategy returns. The process is as follows
\begin{enumerate}[(i)]
    \item At time T, generate 100,000 one-week-ahead random copula shock vectors $z_{T+1}$ according to the conditional copula correlation matrix $\hat{R}_{T+1}$
    \item Transform each $z_{T+1}$ vector to uniform shocks using copula parameters
    \item Transform each uniform shocks vector to GARCH residuals using marginal distribution parameters
    \item Forecast the ARMA-GJR-GARCH process using the residual vectors to get the simulated return vectors $r_{T+1}$
    \item Infer the empirical distribution function of returns using the simulated return vectors
\end{enumerate}
The VaR and ES are then found by
\begin{align}
    VaR_{i,T+1}^{1-\alpha} &= F_{i, T+1}^{-1}(\alpha | I^{sim}_T) \\
    ES_{i, T+1}^{1 - \alpha} &= E_t[r_{i,T+1} | r_{i,T+1} < VaR_{i,T+1}^{1-\alpha}]
\end{align}
where $F_{i, T+1}^{-1}(\alpha | I^{sim}_T)$ is the inverse empirical CDF (based on simulated data set $I^{sim}_t$) of asset $i$ at time $T+1$.

